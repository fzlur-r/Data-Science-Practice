{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageDraw\n",
    "from math import pi, ceil\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from csv\n",
    "df = pd.read_csv('McDonald_Menu_Nutrient_Raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Data Preparation step\n",
    "#look if there any wrong data type\n",
    "print(df.dtypes) #as seen, category column can be treated as category data type\n",
    "print(df['Category'].unique()) #double crosscheck if category is realy unique to be interpreted as category\n",
    "df['Category'] = df['Category'].astype('category') #convert to category data type (so it can be used for multi-indexing)\n",
    "\n",
    "#look if there any duplicated data (Item can be used as primary key) \n",
    "print(df.duplicated(subset = ['Item']).any()) #as seen, \n",
    "\n",
    "#look if there any missing value in dataset\n",
    "DataFrame = df.isnull()\n",
    "col_name = []\n",
    "for column in DataFrame.columns.values.tolist():\n",
    "    dum = DataFrame[column].value_counts().to_dict()\n",
    "    if True in dum:\n",
    "        foo = dum[True]/df_raw.shape[0]\n",
    "        print(\"{}: {:.2f}% data is missing\".format(column, foo))\n",
    "        if foo > 0.33: #threshold for column to drop if more than 33% of its data are missing \n",
    "            col_name.append(column) #this store column name that need to be drop\n",
    "\n",
    "#Creating multi-index with Category as parent index and item as child index            \n",
    "df = df.set_index([df.columns[0], df.columns[1]]) \n",
    "df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_bar_plot(xdata, ydata, xlabel, ylabel, legend, legend_title, color, width = 0.8, legend_loc = 'upper left'):\n",
    "    \"\"\"\n",
    "    #this function to make stacked bar plot\n",
    "    \n",
    "    :xdata -> data in x axis\n",
    "    :ydata -> data in y axis wich a list of collected data in y label\n",
    "    :xlabel -> label name for data in x axis\n",
    "    :ylabel -> label name for data in y axis\n",
    "    :legend -> a list of legend item name\n",
    "    :legend_title -> title for legend\n",
    "    :color -> a list of color palette for data visualization\n",
    "    :width -> bar plot width, initial = 0.8\n",
    "    :legend_loc -> location for lagend position, initial = 'upper left'\n",
    "    \n",
    "    :return: plot figure\n",
    "    \"\"\"\n",
    "    \n",
    "    bottom = np.zeros(len(ydata[0]))\n",
    "    \n",
    "    for i in range(len(ydata)):\n",
    "        plt.bar(xdata, ydata[i], bottom = bottom, color = color[i], edgecolor = 'white', width = width, label = legend[i])\n",
    "        bottom += ydata[i]\n",
    "        \n",
    "    plt.legend(title = legend_title, loc = legend_loc, frameon = False)\n",
    "    plt.xticks(rotation = 45, ha = 'right')\n",
    "\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we visualize the average calories for each menu categories from dataset\n",
    "#get data value for x axis\n",
    "categories = df.index.get_level_values('Category').categories.copy().tolist()\n",
    "\n",
    "#filter column name which like calories (calories & calories from fat)\n",
    "col_name = df.filter(like = 'Calories', axis = 1).columns.values.copy()\n",
    "\n",
    "#get value from filtered column name\n",
    "df_calories = df[col_name].copy()\n",
    "\n",
    "#calculate calories from other source\n",
    "df_calories['Calories from Other'] = df_calories[col_name[0]] - df_calories[col_name[1]]\n",
    "\n",
    "#creating color palete, legend and, label\n",
    "palette = ['#729D7C', '#A5B99A']\n",
    "legend = ['Other', 'Fat']\n",
    "legend_title = 'Source'\n",
    "xlabel = 'Categories'\n",
    "ylabel = 'Calories'\n",
    "\n",
    "#get data value for y axis\n",
    "other_calories = []\n",
    "fat_calories = []\n",
    "for s in categories:\n",
    "    dum = df_calories.loc[s].describe().mean().copy().tolist()\n",
    "    other_calories.append(dum[-1])\n",
    "    fat_calories.append(dum[1])    \n",
    "\n",
    "#preparing data for input\n",
    "ydata = [other_calories, fat_calories]\n",
    "\n",
    "#plotting data\n",
    "plot = stacked_bar_plot(xdata = categories, ydata = ydata, xlabel = xlabel, ylabel = ylabel, legend = legend, legend_title = legend_title, color = palette)\n",
    "\n",
    "# Add a title\n",
    "plot.title('McDonnald Menu\\'s Averages Calories', size=14, y = 1.1)\n",
    "    \n",
    "# Save it\n",
    "filename='calories_average.png'\n",
    "plot.savefig(filename, dpi=96, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we visualize the average fat for each menu categories from dataset\n",
    "#get data value for x axis\n",
    "categories = df.index.get_level_values('Category').categories.copy().tolist()\n",
    "\n",
    "#filter column name which like calories (calories & calories from fat)\n",
    "col_name = df.filter(like = 'Fat', axis = 1).columns.values[1::2].copy()\n",
    "\n",
    "#get value from filtered column name\n",
    "df_fat = df[col_name].copy()\n",
    "\n",
    "#calculate unsaturated fat\n",
    "df_fat['Unsaturated Fat'] = df_fat[col_name[0]] - (df_fat[col_name[1]] + df_fat[col_name[2]])\n",
    "\n",
    "#creating color palete, legend and, label\n",
    "palette = ['#8F2D56', '#58586B', '#218380']\n",
    "legend = ['Unsaturated Fat', 'Saturated Fat', 'Trans Fat']\n",
    "legend_title = 'Type'\n",
    "legend_loc = 'upper right'\n",
    "xlabel = 'Categories'\n",
    "ylabel = 'Fat'\n",
    "\n",
    "#get data value for y axis\n",
    "unsaturated_fat = []\n",
    "saturated_fat = []\n",
    "trans_fat = []\n",
    "for s in categories:\n",
    "    dum = df_fat.loc[s].describe().mean().copy().tolist()\n",
    "    unsaturated_fat.append(dum[-1])\n",
    "    saturated_fat.append(dum[1])\n",
    "    trans_fat.append(dum[2])    \n",
    "\n",
    "#preparing data for input\n",
    "ydata = [unsaturated_fat, saturated_fat, trans_fat]\n",
    "\n",
    "#plotting data\n",
    "plot = stacked_bar_plot(xdata = categories, ydata = ydata, xlabel = xlabel, ylabel = ylabel, legend = legend, legend_title = legend_title, color = palette, legend_loc = legend_loc)\n",
    "\n",
    "# Add a title\n",
    "plot.title('McDonnald Menu\\'s Averages Fat', size=14, y = 1.1)\n",
    "    \n",
    "# Save it\n",
    "filename='fat_average.png'\n",
    "plot.savefig(filename, dpi=96, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(DataFrame, top_n = None, method = 'spearman', remove_duplicates = True, remove_self_correlations = True):\n",
    "    \"\"\"\n",
    "    #Compute the feature correlation and sort feature pairs based on their correlation\n",
    "    \n",
    "    :DataFrame -> The dataframe with the predictor variables\n",
    "    :type DataFrame: pandas.core.frame.DataFrame\n",
    "    :top_n -> Top N feature pairs to be reported (if None, all of the pairs will be returned)\n",
    "    :method -> Correlation compuation method\n",
    "    :type method: str\n",
    "    :remove_duplicates -> Indicates whether duplicate features must be removed\n",
    "    :type remove_duplicates: bool\n",
    "    :remove_self_correlations -> Indicates whether self correlations will be removed\n",
    "    :type remove_self_correlations: bool\n",
    "\n",
    "    :return: pandas.core.frame.DataFrame\n",
    "    \"\"\"\n",
    "    corr_matrix_abs = DataFrame.corr(method=method).abs()\n",
    "    corr_matrix_abs_us = corr_matrix_abs.unstack()\n",
    "    sorted_correlated_features = corr_matrix_abs_us \\\n",
    "        .sort_values(kind=\"quicksort\", ascending=False) \\\n",
    "        .reset_index()\n",
    "\n",
    "    # Remove comparisons of the same feature\n",
    "    if remove_self_correlations:\n",
    "        sorted_correlated_features = sorted_correlated_features[\n",
    "            (sorted_correlated_features.level_0 != sorted_correlated_features.level_1)\n",
    "        ]\n",
    "\n",
    "    # Remove duplicates\n",
    "    if remove_duplicates:\n",
    "        sorted_correlated_features = sorted_correlated_features.iloc[:-2:2]\n",
    "\n",
    "    # Create meaningful names for the columns\n",
    "    sorted_correlated_features.columns = ['Feature 1', 'Feature 2', 'Correlation'] \n",
    "\n",
    "    if top_n:\n",
    "        return sorted_correlated_features[:top_n]\n",
    "\n",
    "    return sorted_correlated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "From dataset, there are similar data like total fat in kcal an total fat in % daily value.\n",
    "we can suspect these variable to be droped later if it's act as redundant data,\n",
    "in which, there are high correlation between each others.\n",
    "There also some variable that's only stated as % daily value, so even it has dependency on daily value need,\n",
    "which we'll discused later, we'll just keep it right now until further analyzing.\n",
    "\n",
    "first wee need to standardize or scaling the data to find their correlation for each others\n",
    "'''\n",
    "def standardize(DataFrame):\n",
    "    #standardize data \n",
    "    std_scale = StandardScaler().fit(DataFrame)\n",
    "    df_std = pd.DataFrame(std_scale.transform(DataFrame))\n",
    "    df_std.index = DataFrame.index.copy()\n",
    "    df_std.columns = DataFrame.columns.copy()\n",
    "    return df_std\n",
    "\n",
    "def normalize(DataFrame):\n",
    "    #normalize data \n",
    "    minmax_scale = MinMaxScaler().fit(DataFrame)\n",
    "    df_minmax = pd.DataFrame(minmax_scale.transform(DataFrame))\n",
    "    df_minmax.index = DataFrame.index.copy()\n",
    "    df_minmax.columns = DataFrame.columns.copy()\n",
    "    return df_minmax\n",
    "\n",
    "df_minmax = normalize(df[df.columns[1:]]) #lets skip the first column since its data types is not numerical\n",
    "df_std = standardize(df[df.columns[1:]])\n",
    "\n",
    "#now look for data corelation\n",
    "corr = correlation(df_std, method = 'spearman') #lets use spearman method because all data is in numerical and they're ordinal/discrete in majority\n",
    "print(\"\\n\" + \"\\033[91m\"+ \"\\033[1m\" + \"Standardize Corelation\" + \"\\033[0m\" + \"\\n\")\n",
    "print(\"\\033[92m\"+ \"\\033[1m\" + \"{}\".format(corr[(corr['Correlation'] >= 0.667) & (corr['Correlation'] < 1.0)]) + \"\\033[0m\") #only show data that has corelation with calories\n",
    "\n",
    "#now what if we use normalized data instead of standardize one to look for data correlation\n",
    "corr = correlation(df_minmax, method = 'spearman')\n",
    "print(\"\\n\" + \"\\033[94m\"+ \"\\033[1m\" + \"Normalize Corelation\" + \"\\033[0m\" + \"\\n\")\n",
    "print(\"\\033[36m\"+ \"\\033[1m\" + \"{}\".format(corr[(corr['Correlation'] >= 0.667) & (corr['Correlation'] < 1.0)]) + \"\\033[0m\") #only show data that has corelation with calories\n",
    "\n",
    "'''\n",
    "from what we get, using normalize or standardize for finding corelation in this dataset seems not to be an issue.\n",
    "also we can treated feature that has similarity and high corelation for each other as redundant data and drop it.\n",
    "redundant data wich stated as % daily value will be droped because it has dependency for other feature that's not in this dataset.\n",
    "but let's keep feature like vitamin a, vitamin c, calcium and iron even if it's in % daily value because some value have cross corelation which is good if we want to make prediction or something.\n",
    "also we'll drop saturated fat, trans fat, and calories from fat and keep Total fat and Total calories instead.\n",
    "for better analizing we'll drop Serving Size columns\n",
    "'''\n",
    "\n",
    "#now we drop another redundant/similar data\n",
    "df = df[df.columns[1:]].drop(df.filter(like = '%').columns[:-4], axis = 1) #be carefull if you need to run this again in jupyter make it a commen or it'll raised error since the column name has been droped from the previous one. as for spyder user it's not an issue seen all syntax will be run from the start.\n",
    "df.drop(['Calories from Fat', 'Saturated Fat', 'Trans Fat'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spider_plot(DataFrame, num = None, label = None, color = [None, None, None, None], maxval = None):\n",
    "    '''\n",
    "    #this function make spider plot and save it\n",
    "    \n",
    "    :DataFrame -> data to plot\n",
    "    :num -> number for figure save file\n",
    "    :label -> figure label name\n",
    "    '''\n",
    "    # Set data \n",
    "    df = DataFrame\n",
    "    str1, str2 = df.index.values\n",
    "    str1 = np.asarray(str1.split(\" \"))\n",
    "    str2 = np.asarray(str2.split(\" \"))\n",
    "    legend_label = [' '.join([s for s in str1 if (s not in str2)]), ' '.join([s for s in str2 if (s not in str1)])] if label is None else label\n",
    "    # ------- PART 1: Create background\n",
    " \n",
    "    # number of variable\n",
    "    categories=list(df)\n",
    "    N = len(categories)\n",
    " \n",
    "    # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    " \n",
    "    # Initialise the spider plot\n",
    "    ax = plt.subplot(111, polar=True)\n",
    " \n",
    "    # If you want the first axis to be on top:\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    " \n",
    "    # Draw one axe per variable + add labels labels yet\n",
    "    plt.xticks(angles[:-1], categories, color = color[2])\n",
    "    ticks = np.linspace(0, 2*np.pi, N, endpoint=False)\n",
    "    for label,rot,i in zip(ax.get_xticklabels(),ticks, range(N)):\n",
    "        if i % int(N/2) != 0:\n",
    "            if i // int(N/2) == 0:\n",
    "                label.set_horizontalalignment(\"left\")\n",
    "            else:\n",
    "                label.set_horizontalalignment(\"right\")\n",
    "\n",
    "    # Draw ylabels\n",
    "    maxval = ceil(df.max().max()*10)/10 if maxval == None else maxval\n",
    "    label = np.linspace(0,maxval,5)[1:-1]\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks(label, list(filter(None, str(label).strip('[]').split(' '))), color=\"grey\", size=7)\n",
    "    plt.ylim(0,maxval)\n",
    " \n",
    "    # ------- PART 2: Add plots\n",
    " \n",
    "    # Plot each individual = each line of the data\n",
    "\n",
    "    # Ind1\n",
    "    values=df.iloc[0].values.flatten().tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, color = color[0], linewidth=1, linestyle='solid', label=legend_label[0])\n",
    "    ax.fill(angles, values, color = color[0], alpha=0.2)\n",
    " \n",
    "    # Ind2\n",
    "    values=df.iloc[1].values.flatten().tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, color = color[1], linewidth=1, linestyle='solid', label=legend_label[1])\n",
    "    ax.fill(angles, values, color = color[1], alpha=0.2)\n",
    "\n",
    "    # Add a title\n",
    "    plt.title(' '.join([s for s in str2 if (s in str1)]), size=14, color=color[3], y = 1.2)\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend(bbox_to_anchor=(1.45, 1.15))\n",
    "    #plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "\n",
    "    # Save it\n",
    "    filename=legend_label[0]+'_'+legend_label[1]+str(num+1)+'.png'\n",
    "    plt.savefig(filename, dpi=96, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we'll see how food feature affect the food nutrients\n",
    "#filter data to compare grilled chicken vs crispy chicken in sandwich can affect food nutrients\n",
    "df_minmax = normalize(df)\n",
    "df_dummy = df_minmax.filter(like = 'Sandwich', axis = 0).copy()\n",
    "df1 = df_dummy.filter(like = 'Grilled Chicken', axis = 0).copy()\n",
    "df2 = df_dummy.filter(like = 'Crispy Chicken', axis = 0).copy()\n",
    "\n",
    "images = []\n",
    "\n",
    "#make spider plot\n",
    "color = ['#3B3923', '#BCB076', '#8F4020', '#4C061D']\n",
    "for i in range(min(df1.shape[0], df2.shape[0])):\n",
    "    spider_plot(pd.concat([df1.iloc[i:i+1], df2.iloc[i:i+1]]).sort_index().droplevel(level = 0), num = i, color = color, maxval = 0.6)\n",
    "\n",
    "for i in range(min(df1.shape[0], df2.shape[0])):\n",
    "    im = Image.open(\"Crispy_Grilled\"+str(i+1)+\".png\")    \n",
    "    images.append(im)\n",
    "\n",
    "images[0].save('Crispy_Grilled.gif', save_all=True, append_images=images[1:], optimize=False, duration=1000, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we'll see how food feature affect the food nutrients\n",
    "#filter data to compare grilled chicken vs crispy chicken in sandwich can affect food nutrients\n",
    "df_minmax = normalize(df)\n",
    "df_dummy = df_minmax.filter(like = 'Egg', axis = 0).droplevel(level = 0).copy()\n",
    "df2 = df_dummy.filter(like = 'Egg White', axis = 0).copy()\n",
    "df1 = df_dummy.drop(df2.index).copy()\n",
    "\n",
    "#manual filter to drop uncomparable menus\n",
    "df1 = df1.drop([df1.index[0], df1.index[2], df1.index[7], df1.index[-1]], axis = 0)\n",
    "df2 = df2.iloc[1:9]\n",
    "\n",
    "images = []\n",
    "\n",
    "#make spider plot\n",
    "color = ['#5B3758', '#83B692', '#C65B7C', '#F9888F']\n",
    "for i in range(min(df1.shape[0], df2.shape[0])):\n",
    "    spider_plot(pd.concat([df1.iloc[i:i+1], df2.iloc[i:i+1]]).sort_index(), num = i, label = ['Whole Egg', 'Egg White'], color = color, maxval = 0.6)\n",
    "    \n",
    "for i in range(min(df1.shape[0], df2.shape[0])):\n",
    "    im = Image.open(\"Whole Egg_Egg White\"+str(i+1)+\".png\")    \n",
    "    images.append(im)\n",
    "\n",
    "images[0].save('Whole Egg_Egg White.gif', save_all=True, append_images=images[1:], optimize=False, duration=1000, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#last let's see how each feature corelated each other by plotting each of them in scattered pairplod\n",
    "#for this one, only plot feature that has cross corelation (fat corelated to calories, and calories corelated to sodium, and sodium corelated to fat, etc)\n",
    "df_std = standardize(df)\n",
    "df_std = df_std.reset_index().set_index('Item')\n",
    "sns_plot = sns.pairplot(df_std[['Category', 'Protein', 'Calories', 'Total Fat', 'Sodium', 'Cholesterol', 'Carbohydrates']], kind=\"scatter\", hue='Category', palette=\"Set2\")\n",
    "sns_plot.savefig(\"Feature_Pairplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's save it for now and let's analize it further next time\n",
    "df.to_csv('McDonald_Menu_Nutrient.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
