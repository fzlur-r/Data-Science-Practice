{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this read dataset\n",
    "df_menu = pd.read_csv('McDonald_Menu_Nutrient.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Below to Evaluate what Machine Learning Algorithm that Best Suite for The First Dataset, McDOnald Menu Nutrient Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#let's have a look on main dataset, McDonald Menu Nutrient\n",
    "df_menu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now what we can do about the dataset? as far as we know, these feature have high corelation, \n",
    "thus making prediction about menu nutrient seems possible, but that's not the problem we want to explore.\n",
    "By looking at these dataset, clustering might be done but that only good for excercise, \n",
    "since these menu have been categorized, clustering become unecessary.\n",
    "Next option we can also categorized the dataset into new category such as junk food, healthy food etc, \n",
    "but again, we want go deeper.\n",
    "\n",
    "How about we make menu recomendation by making prediction model? some people have their diet procedure, \n",
    "but still want cheating their agenda and enjoy everything good in restaurant. Since we have these dataset,\n",
    "we can tell people who's young, teen, adults, or old; man or woman; or whoever that like to excercise, \n",
    "doing hard works or just laying on bed about their diet menu. People often have daily nutrients need and\n",
    "we like to help them choosing the menu without worry about over-nutrient.\n",
    "\n",
    "for the first step, let's we prepare these data and make a model by train them with dataset. for the output, \n",
    "we'll chose menu item. since those menus are in category, we'll make the category as input too. thus the user\n",
    "will be free to chose they want smoothies & shake in their dietary program or not.\n",
    "\n",
    "after choosing the machine learning algorithm, we'll add another data set so we can connect user information\n",
    "into this nutrient menu dataset to make recomendation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(DataFrame, top_n = None, method = 'spearman', remove_duplicates = True, remove_self_correlations = True):\n",
    "    \"\"\"\n",
    "    #Compute the feature correlation and sort feature pairs based on their correlation\n",
    "    \n",
    "    :DataFrame -> The dataframe with the predictor variables\n",
    "    :type DataFrame: pandas.core.frame.DataFrame\n",
    "    :top_n -> Top N feature pairs to be reported (if None, all of the pairs will be returned)\n",
    "    :method -> Correlation compuation method\n",
    "    :type method: str\n",
    "    :remove_duplicates -> Indicates whether duplicate features must be removed\n",
    "    :type remove_duplicates: bool\n",
    "    :remove_self_correlations -> Indicates whether self correlations will be removed\n",
    "    :type remove_self_correlations: bool\n",
    "\n",
    "    :return: pandas.core.frame.DataFrame\n",
    "    \"\"\"\n",
    "    corr_matrix_abs = DataFrame.corr(method=method).abs()\n",
    "    corr_matrix_abs_us = corr_matrix_abs.unstack()\n",
    "    sorted_correlated_features = corr_matrix_abs_us \\\n",
    "        .sort_values(kind=\"quicksort\", ascending=False) \\\n",
    "        .reset_index()\n",
    "\n",
    "    # Remove comparisons of the same feature\n",
    "    if remove_self_correlations:\n",
    "        sorted_correlated_features = sorted_correlated_features[\n",
    "            (sorted_correlated_features.level_0 != sorted_correlated_features.level_1)\n",
    "        ]\n",
    "\n",
    "    # Remove duplicates\n",
    "    if remove_duplicates:\n",
    "        sorted_correlated_features = sorted_correlated_features.iloc[:-2:2]\n",
    "\n",
    "    # Create meaningful names for the columns\n",
    "    sorted_correlated_features.columns = ['Feature 1', 'Feature 2', 'Correlation'] \n",
    "\n",
    "    if top_n:\n",
    "        return sorted_correlated_features[:top_n]\n",
    "\n",
    "    return sorted_correlated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(DataFrame):\n",
    "    #standardize data \n",
    "    std_scale = StandardScaler().fit(DataFrame)\n",
    "    df_std = pd.DataFrame(std_scale.transform(DataFrame))\n",
    "    df_std.index = DataFrame.index.copy()\n",
    "    df_std.columns = DataFrame.columns.copy()\n",
    "    return df_std\n",
    "\n",
    "def normalize(DataFrame):\n",
    "    #normalize data \n",
    "    minmax_scale = MinMaxScaler().fit(DataFrame)\n",
    "    df_minmax = pd.DataFrame(minmax_scale.transform(DataFrame))\n",
    "    df_minmax.index = DataFrame.index.copy()\n",
    "    df_minmax.columns = DataFrame.columns.copy()\n",
    "    return df_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we make category as onehot model and concate them with our dataset\n",
    "#df_menu = pd.read_csv('McDonald_Menu_Nutrient.csv')\n",
    "df_menu = pd.concat([df_menu, pd.get_dummies(df_menu['Category'])], axis = 1)\n",
    "\n",
    "#now since item is unique id, it's not wise to make them as onehot model too.\n",
    "#binning them into small category will make us lost our purpose since we want get the exact item name not it's approximation\n",
    "#since the item is not in some kind of order (not comparable like color) still we can rank it based on it's feature.\n",
    "#here we're talking about calories. so we rank menu item based on them, and if it's tie, we compare them with other feature which highly correlated with calories\n",
    "df_menu = df_menu.set_index(['Category', 'Item']) #make this code as comment if it's been run 1 because it'll raise error in jupyter since dataframe have been change. but it'll be okay if run in spyder\n",
    "df_minmax = normalize(df_menu) #lets skip the first two column since its data types is not numerical\n",
    "df_std = standardize(df_menu)\n",
    "\n",
    "#looking for feature that high correlated to calories\n",
    "corr = correlation(df_minmax, method = 'spearman') #lets use spearman method because all data is in numerical and they're ordinal/discrete in majority\n",
    "print(\"\\n\" + \"\\033[94m\"+ \"\\033[1m\" + \"Feature Corelation\" + \"\\033[0m\" + \"\\n\")\n",
    "print(\"\\033[92m\"+ \"\\033[1m\" + \"{}\".format(corr[(corr['Correlation'] >= 0.667) & (corr['Correlation'] < 1.0)])) #only show data that has corelation with calories\n",
    "\n",
    "#Rank the item in each category\n",
    "df_menu['Product'] = 1\n",
    "col_name = ['Calories'] + corr[(corr['Feature 1'] == 'Calories') | (corr['Feature 1'] == 'Calories')]['Feature 2'].values.copy().tolist()\n",
    "for ix in df_menu.index.get_level_values('Category').unique().values.copy().tolist():\n",
    "    df_menu.loc[ix] = df_menu.loc[ix].sort_values(col_name).values.copy()\n",
    "    df_menu.loc[ix, 'Product'] = np.arange(1, df_menu.loc[ix].shape[0]+1)\n",
    "    df_menu.loc[ix] = df_menu.loc[ix].sort_index(axis = 0, level = 'Item').values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = df_menu.columns.values.copy().tolist()\n",
    "col_name = col_name[:-9] + [col_name[-1]]\n",
    "df_minmax = normalize(df_menu[col_name])\n",
    "#looking for feature that high correlated to calories\n",
    "corr = correlation(df_minmax, method = 'spearman') #lets use spearman method because all data is in numerical and they're ordinal/discrete in majority\n",
    "print(\"\\n\" + \"\\033[94m\"+ \"\\033[1m\" + \"Feature Corelation\" + \"\\033[0m\" + \"\\n\")\n",
    "print(\"\\033[92m\"+ \"\\033[1m\" + \"{}\".format(corr[(corr['Correlation'] >= 0.5) & (corr['Correlation'] < 1.0)]) + \"\\033[0m\") #only show data that has corelation with calories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After looking at the correlation, now let's create machine learning model.\n",
    "Since what we aim is making menu recommendation, thus we need model for prediction such as regression or neural network.\n",
    "\n",
    "Let's start with the easier one, a regression model.\n",
    "because there're so many independent variable, then we need to use multiple linear regression model.\n",
    "we're choosing linear instead of non-linear, because each independent variable shows strong correlation,\n",
    "and product have good corelation with some feature. after that, let's we compare \n",
    "when the model use normalize dataset and when the model use stadardize dataset.\n",
    "\n",
    "Next let's try modeling with neural network anda make comparation like befor.\n",
    "we also compare machine learning algorithm that used for modelling. The best one will show minimal mse\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi regression for predict product item using normalize dataset\n",
    "df_minmax = normalize(df_menu)\n",
    "col_name = df_minmax.columns.values.copy().tolist()\n",
    "x = df_minmax[col_name[:-1]] # here we have 2 variables for multiple regression. If you just want to use one variable for simple linear regression, then use X = df['Interest_Rate'] for example.Alternatively, you may add additional variables within the brackets\n",
    "y = df_minmax['Product']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=4)\n",
    "\n",
    "# with sklearn\n",
    "regr = LinearRegression()\n",
    "regr.fit(x_train, y_train)\n",
    "print(\"\\033[95m\"+ \"\\033[1m\" + \"Means Squared Error when Training using Normalize Dataset\" + \"\\033[0m\" + \"\\n\")\n",
    "print(\"\\033[94m\"+ \"\\033[1m\" + \"Intercept: \\n\" + \"\\033[36m\"+ \"\\033[1m\" + \"{}\".format(regr.intercept_) + \"\\033[0m\" + \"\\n\")\n",
    "print(\"\\033[94m\"+ \"\\033[1m\" + \"Coefficients: \\n\" + \"\\033[36m\"+ \"\\033[1m\" + \"{}\".format(regr.coef_) + \"\\033[0m\" + \"\\n\")\n",
    "\n",
    "# prediction with sklearn\n",
    "Product_pred = [regr.predict([val]) for val in x_test.values.copy().tolist()]\n",
    "print(\"\\033[94m\"+ \"\\033[1m\" + \"MSE: \\n\" + \"\\033[36m\"+ \"\\033[1m\" + \"{}\".format(mean_squared_error(y_test.values.copy().tolist(), Product_pred)) + \"\\033[0m\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi regression for predict product item using standardize dataset\n",
    "df_std = standardize(df_menu)\n",
    "col_name = df_std.columns.values.copy().tolist()\n",
    "x = df_std[col_name[:-1]] # here we have 2 variables for multiple regression. If you just want to use one variable for simple linear regression, then use X = df['Interest_Rate'] for example.Alternatively, you may add additional variables within the brackets\n",
    "y = df_std['Product']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=4)\n",
    "\n",
    "# with sklearn\n",
    "regr = LinearRegression()\n",
    "regr.fit(x_train, y_train)\n",
    "print(\"\\033[93m\"+ \"\\033[1m\" + \"Means Squared Error when Training using Standardize Dataset\" + \"\\033[0m\" +\"\\n\")\n",
    "print(\"\\033[91m\"+ \"\\033[1m\" + \"Intercept: \\n\" + \"\\033[92m\"+ \"\\033[1m\" + \"{}\".format(regr.intercept_) + \"\\033[0m\" + \"\\n\")\n",
    "print(\"\\033[91m\"+ \"\\033[1m\" + \"Coefficients: \\n\" + \"\\033[92m\"+ \"\\033[1m\" + \"{}\".format(regr.coef_) + \"\\033[0m\" + \"\\n\")\n",
    "\n",
    "# prediction with sklearn\n",
    "Product_pred = [regr.predict([val]) for val in x_test.values.copy().tolist()]\n",
    "print(\"\\033[91m\"+ \"\\033[1m\" + \"MSE: \\n\" + \"\\033[92m\"+ \"\\033[1m\" + \"{}\".format(mean_squared_error(y_test.values.copy().tolist(), Product_pred)) + \"\\033[0m\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural network for predict product item using normalize dataset\n",
    "df_minmax = normalize(df_menu)\n",
    "\n",
    "#select feature for independent variable\n",
    "col_name = df_menu.columns[:-1].values.copy().tolist()\n",
    "x = np.asarray(df_minmax[col_name])\n",
    "\n",
    "#select feature for dependent variable\n",
    "col_name = 'Product'\n",
    "y = np.asarray(df_minmax[col_name])\n",
    "\n",
    "hlayer_row = []\n",
    "hlayer_col = []\n",
    "train_mse = []\n",
    "test_mse = []\n",
    "\n",
    "for i in np.arange(1, 21):\n",
    "    for j in np.arange(1, 21):\n",
    "        hlayer_row.append(i)\n",
    "        hlayer_col.append(j)\n",
    "        #split data for training and testing\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=4)\n",
    "        #Train Model and Predict\n",
    "        net = MLPRegressor(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(i, j), random_state=1)\n",
    "        #clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(i, j), random_state=1)\n",
    "\n",
    "        net.fit(x_train, y_train)\n",
    "\n",
    "        train_mse.append(mean_squared_error(y_train, net.predict(x_train)))\n",
    "        test_mse.append(mean_squared_error(y_test, net.predict(x_test)))\n",
    "        \n",
    "df_net_minmax=pd.DataFrame({'Hidden Layer Row': hlayer_row, 'Hidden Layer Column': hlayer_col, 'Train MSE': train_mse, 'Test MSE': test_mse})\n",
    "print(\"\\033[94m\"+ \"\\033[1m\" + \"Means Squared Error when Training using Normalize Dataset\" + \"\\033[0m\" + \"\\n\")\n",
    "print(\"\\033[36m\"+ \"\\033[1m\" + \"{}\".format(df_net_minmax.sort_values(['Test MSE', 'Train MSE']).head()) + \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural network for predict product item using standardize dataset\n",
    "df_std = standardize(df_menu)\n",
    "\n",
    "#select feature for independent variable\n",
    "col_name = df_menu.columns[:-1].values.copy().tolist()\n",
    "x = np.asarray(df_std[col_name])\n",
    "\n",
    "#select feature for dependent variable\n",
    "col_name = 'Product'\n",
    "y = np.asarray(df_std[col_name])\n",
    "\n",
    "hlayer_row = []\n",
    "hlayer_col = []\n",
    "train_mse = []\n",
    "test_mse = []\n",
    "\n",
    "for i in np.arange(1, 21):\n",
    "    for j in np.arange(1, 21):\n",
    "        hlayer_row.append(i)\n",
    "        hlayer_col.append(j)\n",
    "        #split data for training and testing\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=4)\n",
    "        #Train Model and Predict\n",
    "        net = MLPRegressor(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(i, j), random_state=1)\n",
    "        #clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(i, j), random_state=1)\n",
    "\n",
    "        net.fit(x_train, y_train)\n",
    "\n",
    "        train_mse.append(mean_squared_error(y_train, net.predict(x_train)))\n",
    "        test_mse.append(mean_squared_error(y_test, net.predict(x_test)))\n",
    "        \n",
    "df_net_std=pd.DataFrame({'Hidden Layer Row': hlayer_row, 'Hidden Layer Column': hlayer_col, 'Train MSE': train_mse, 'Test MSE': test_mse})\n",
    "print(\"\\033[91m\"+ \"\\033[1m\" + \"Means Squared Error when Training using Standardize Dataset\" + \"\\033[0m\" +\"\\n\")\n",
    "print(\"\\033[92m\"+ \"\\033[1m\" + \"{}\".format(df_net_std.sort_values(['Test MSE', 'Train MSE']).head()) + \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Below to Prepared the Additional Dataset that Needed to Make Food Recommendation for Daily Nutrients Need Fullfilment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "As seen, using normalize dataset will give better performance in model training,\n",
    "also even learning method can be perform with linear regression, neural network still give better prediction,\n",
    "because it give results with minimal error. we also get training parameter that best fit when using neural network,\n",
    "where we using (9x20) hidden layer.\n",
    "\n",
    "Now let's prepare other data set so we can make deployment easier. we won't expect for user to input their nutrients need\n",
    "one by one, sp we make prediction for that too. we need to know calory needs based on user age, sex, and activity.\n",
    "after that we need to know other food nutrients when we only know user calories. based on what we've done before, using\n",
    "linear regression to find other corelated food nutrient will make the solution.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's import the dataset\n",
    "#this dataset used for predict user calory needs\n",
    "df_fcal = pd.read_csv('Female_Calories_Needs.csv')\n",
    "df_mcal = pd.read_csv('Male_Calories_Needs.csv')\n",
    "\n",
    "#this dataset used for predict user nutrient food other than calory\n",
    "df_nut = pd.read_csv('Daily_Nutrients_Needs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[94m' + '\\033[1m' + '{}'.format(df_mcal.head()) + '\\033[0m' + '\\n')\n",
    "print('\\033[91m' + '\\033[1m' + '{}'.format(df_fcal.head()) + '\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nut.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(DataFrame, bin_column = 'Age'):\n",
    "    #Corecting Column names\n",
    "    col_name = DataFrame.columns.values.copy()\n",
    "    col_name[0] = col_name[0].capitalize()\n",
    "    col_name[1:] = range(3)\n",
    "    DataFrame.columns = col_name\n",
    "    \n",
    "    #Correcting age value\n",
    "    DataFrame[bin_column] = DataFrame[bin_column].str[:2]\n",
    "    \n",
    "    #remove comma separator\n",
    "    DataFrame = pd.DataFrame([DataFrame[col].str.replace(',','') for col in DataFrame.columns.values.copy()]).transpose()\n",
    "    \n",
    "    #data type correction\n",
    "    DataFrame = DataFrame.astype('int64')\n",
    "    \n",
    "    #binning age category\n",
    "    bins = np.arange(1,100,5)\n",
    "    labels = np.arange(1,len(bins-1))\n",
    "    DataFrame[bin_column+'_Bin'] = pd.cut(DataFrame[bin_column], bins=bins, labels=labels)\n",
    "    DataFrame[bin_column+'_Bin'] = DataFrame[bin_column+'_Bin'].astype('int64')\n",
    "    \n",
    "    DataFrame = DataFrame.groupby(bin_column+'_Bin').mean()\n",
    "    \n",
    "    #data type correction\n",
    "    DataFrame = DataFrame.astype('int64')\n",
    "    \n",
    "    #drop redundant column\n",
    "    DataFrame.drop(bin_column, axis = 1, inplace = True)\n",
    "    \n",
    "    #creating activity category\n",
    "    DataFrame = pd.DataFrame(DataFrame.stack()).reset_index()\n",
    "    DataFrame = DataFrame.rename(columns={'level_1': 'Activity', 0: 'Calories'})\n",
    "    \n",
    "    return DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mcal = prepare(df_mcal)\n",
    "df_mcal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fcal = prepare(df_fcal)\n",
    "df_fcal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking(DataFrame, num_stack, col_name):\n",
    "    #creating activity category\n",
    "    DataFrame = DataFrame.set_index(DataFrame.columns[:-num_stack].values.copy().tolist()) #this finish data preparation\n",
    "    DataFrame = pd.DataFrame(DataFrame.stack()).reset_index()\n",
    "    DataFrame = DataFrame.rename(columns={DataFrame.columns[-2]: col_name})\n",
    "    DataFrame.drop(0, axis = 1, inplace = True)\n",
    "    \n",
    "    return DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mcal[1] = 1\n",
    "df_fcal[0] = 1\n",
    "df_cal = pd.concat([df_mcal, pd.DataFrame(df_fcal[0])], axis = 1)\n",
    "\n",
    "df_cal = stacking(df_cal, 2, col_name = 'Sex')\n",
    "df_cal = df_cal.set_index(['Sex', 'Age_Bin', 'Activity']).sort_index()\n",
    "df_cal = df_cal.reset_index().set_index(['Sex'])\n",
    "\n",
    "df_fcal = df_fcal.set_index(['Age_Bin', 'Activity']).sort_index()\n",
    "df_fcal = df_fcal.reset_index()\n",
    "\n",
    "df_cal.loc[0, 'Calories'] = df_fcal['Calories'].values.copy()\n",
    "df_cal = df_cal.reset_index()\n",
    "df_cal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "foot note:\n",
    "the calories are in kcal\n",
    "the sex is 0 for female and 1 for male\n",
    "the age_bin are 1 for 1-5 yr, 2 for 5-10 yr, . . ., 15 for 51++ (if i'm not mistaken)\n",
    "the activity is 0 for Sedentary, 1 for moderately active, and 2 for active\n",
    "\n",
    "next we prepare nutrient data needs for dietary people (i mean daily need)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nut = pd.read_csv('dietary.csv')\n",
    "df_nut = df_nut.transpose()\n",
    "df_nut = df_nut.rename(columns=df_nut.iloc[0])\n",
    "df_nut.drop(df_nut.index[0:2], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look if there any missing value in dataset\n",
    "DataFrame = df_nut.isnull().copy()\n",
    "col_name = []\n",
    "for column in DataFrame.columns.values.copy().tolist():\n",
    "    dum = DataFrame[column].value_counts().to_dict()\n",
    "    if True in dum:\n",
    "        foo = dum[True]/df_nut.shape[0]\n",
    "        print(\"{}: {:.2f}% data is missing\".format(column, foo))\n",
    "        if foo > 0.33: #threshold for column to drop if more than 33% of its data are missing \n",
    "            col_name.append(column) #this store column name that need to be drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unused nutrient info\n",
    "#macronutrient, minerals and vitamins are category name but in bad format, that's why all value is missing\n",
    "#but we drop those still since we don't need it.\n",
    "df_nut.drop(col_name + ['Protein,\\xa0%\\xa0kcal', 'Carbohydrate, %\\xa0kcal', 'Saturated fat, %\\xa0kcal', 'Linoleic acid, g', \n",
    "                       'Linolenic acid, g', 'Magnesium, mg', 'Phosphorus, mg', 'Potassium, mg', 'Zinc, mg', \n",
    "                       'Copper, mcg', 'Manganese, mg', 'Selenium, mcg', 'Vitamin E, mg\\xa0AT', 'Vitamin D, IU', \n",
    "                       'Thiamin, mg', 'Riboflavin, mg', 'Niacin, mg', 'Vitamin B6, mg', 'Vitamin B12, mcg', \n",
    "                       'Choline, mg', 'Vitamin K, mcg', 'Folate, mcg\\xa0DFE'], axis = 1, inplace = True)\n",
    "\n",
    "#rename columns name\n",
    "df_nut = df_nut.reset_index()\n",
    "col_name = ['Sex_Age', 'Calories (kcal)', 'Protein (g)', 'Carbohydrate (g)', 'Dietary fiber (g)', 'Sugars (% kcal)', \n",
    "            'Total Fat (% kcal)', 'Calcium (mg)', 'Iron (mg)', 'Sodium (mg)', 'Vitamin A (mcg)', 'Vitamin C (mg)']\n",
    "df_nut.columns = col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's do some data preparation\n",
    "df_nut['Calcium (mg)'] = df_nut['Calcium (mg)'].str.replace(',', '')\n",
    "df_nut['Sodium (mg)'] = df_nut['Sodium (mg)'].str.replace(',', '')\n",
    "\n",
    "df_nut['Sugars (% kcal)'] = df_nut['Sugars (% kcal)'].str.replace('<', '').str.replace('%', '')\n",
    "\n",
    "df_dummy = df_nut['Total Fat (% kcal)'].str.split('-', expand = True)\n",
    "df_dummy.columns = ['Fat_lo', 'Fat_up']\n",
    "df_nut = pd.concat([df_nut, df_dummy], axis = 1)\n",
    "\n",
    "df_dummy = df_nut['Sex_Age'].str.split(' ', expand = True)\n",
    "df_dummy.columns = ['Sex', 'Age']\n",
    "df_nut = pd.concat([df_nut, df_dummy], axis = 1)\n",
    "\n",
    "df_nut = df_nut.append(df_nut.iloc[0].copy(), ignore_index = True)\n",
    "df_nut['Sex'][df_nut['Sex'] == 'Child'] = ['Female', 'Male']\n",
    "row = df_nut.index.values.copy().tolist()\n",
    "row = [row[0], row[-1]] + row[1:-1]\n",
    "df_nut = df_nut.iloc[row]\n",
    "df_nut.drop('Sex_Age', axis = 1, inplace = True)\n",
    "df_nut = df_nut.set_index('Sex')\n",
    "\n",
    "df_dummy = df_nut['Age'].str.split('[+|-]', expand = True)\n",
    "df_dummy.columns = ['Age_lo', 'Age_up']\n",
    "df_dummy.iloc[-2:, df_dummy.columns.get_loc('Age_up')] = df_dummy.iloc[-2:, df_dummy.columns.get_loc('Age_up')].str.replace('', '51')\n",
    "df_nut = pd.concat([df_nut, df_dummy], axis = 1)\n",
    "\n",
    "df_nut['Calories_lo'] = df_nut['Calories (kcal)'].str[:5].copy()\n",
    "df_nut['Calories_up'] = df_nut['Calories (kcal)'].str[-5:].copy()\n",
    "df_nut['Calories_lo'] = df_nut['Calories_lo'].str.replace(',', '')\n",
    "df_nut['Calories_up'] = df_nut['Calories_up'].str.replace(',', '')\n",
    "\n",
    "df_nut['Calcium (mg)'] = df_nut['Calcium (mg)'].str.replace('b', '')\n",
    "\n",
    "name = ['Calories (kcal)', 'Protein (g)', 'Total Fat (% kcal)', 'Age']\n",
    "sub = ['Calories', 'Protein', 'Fat', 'Age']\n",
    "\n",
    "col_name = df_nut.filter(like = '_lo', axis = 1).columns.values.copy().tolist()\n",
    "col_name += df_nut.filter(like = '_up', axis = 1).columns.values.copy().tolist()\n",
    "df_nut[col_name] = df_nut[col_name].astype('int64')\n",
    "\n",
    "for i in range(len(name)):\n",
    "    df_nut[name[i]] = df_nut[df_nut.filter(like = sub[i], axis = 1).columns.values.copy().tolist()[-2:]].mean(axis=1)\n",
    "    \n",
    "df_nut.drop(col_name, axis = 1, inplace = True)\n",
    "\n",
    "#binning age category\n",
    "bins = np.arange(1,100,5)\n",
    "labels = np.arange(1,len(bins-1))\n",
    "df_nut['Age_Bin'] = pd.cut(df_nut['Age'], bins=bins, labels=labels)\n",
    "df_nut['Age_Bin'] = df_nut['Age_Bin'].astype('int64')\n",
    "\n",
    "col_name = df_nut.columns.values.copy().tolist()\n",
    "col_name = [col_name[-1]] + col_name[0:-2]\n",
    "df_nut = df_nut[col_name]\n",
    "\n",
    "df_nut = df_nut.reset_index()\n",
    "df_nut['Sex'] = df_nut['Sex'].map({'Female': 0, 'Male': 1})\n",
    "\n",
    "df_nut['Dietary fiber (g)'] = df_nut['Dietary fiber (g)'].astype('float64')\n",
    "df_nut[df_nut.columns.difference(['Dietary fiber (g)'])] = df_nut[df_nut.columns.difference(['Dietary fiber (g)'])].astype('int64')\n",
    "df_nut.head()\n",
    "#df.to_csv('Diet_Prepared.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Files\n",
    "df_menu.to_csv('McDonald_Menu_Nutrients_Clean.csv')\n",
    "df_cal.to_csv('Daily_Calory_Needs_Clean.csv', index = False)\n",
    "df_nut.to_csv('Daily_Nutrients_Needs_Clean.csv')\n",
    "\n",
    "\"\"\"\"\n",
    "#age binning info\n",
    "    :bins = np.arange(1,100,5)\n",
    "    :labels = np.arange(1,len(bins-1))\n",
    "    :DataFrame[bin_column+'_Bin'] = pd.cut(DataFrame[bin_column], bins=bins, labels=labels)\n",
    "\n",
    "#sex -> 0 = Female; 1 = Male\n",
    "\n",
    "#Activities -> 0: Sedentary\n",
    "               1: Moderately Active\n",
    "               2: Active\n",
    "\n",
    "#use this if you need some creativity with text output\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "print(color.BOLD + 'Hello World !' + color.END)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
